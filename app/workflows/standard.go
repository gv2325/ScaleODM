package workflows

import (
	"context"
	"fmt"
	"strings"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	wfv1 "github.com/argoproj/argo-workflows/v3/pkg/apis/workflow/v1alpha1"
	apiv1 "k8s.io/api/core/v1"

	"github.com/hotosm/scaleodm/app/config"
	"github.com/hotosm/scaleodm/app/s3"
)

// ODMPipelineConfig holds configuration for ODM pipeline workflow
type ODMPipelineConfig struct {
	ODMProjectID   string
	ReadS3Path     string   // S3 path where raw imagery is located (can contain zips)
	WriteS3Path    string   // S3 path where final ODM outputs will be written
	ODMFlags       []string // ODM command line flags
	S3Region       string
	S3Endpoint     string            // Optional custom S3 endpoint for non-AWS providers
	S3Credentials  *s3.S3Credentials // S3 credentials for the workflow
	ServiceAccount string
	RcloneImage    string
	ODMImage       string
}

// NewDefaultODMConfig returns default configuration
// Note: S3Credentials must be set separately before creating the workflow (always required)
func NewDefaultODMConfig(odmProjectID, readS3Path, writeS3Path string, odmFlags []string) *ODMPipelineConfig {
	return &ODMPipelineConfig{
		ODMProjectID:   odmProjectID,
		ReadS3Path:     readS3Path,
		WriteS3Path:    writeS3Path,
		ODMFlags:       odmFlags,
		S3Region:       "us-east-1",
		S3Endpoint:     "",
		S3Credentials:  nil, // Must be set before creating workflow (always required)
		ServiceAccount: "argo-odm",
		RcloneImage:    "docker.io/rclone/rclone:1",
		ODMImage:       config.SCALEODM_ODM_IMAGE,
	}
}

// CreateODMWorkflow creates and submits an ODM processing workflow
func (c *Client) CreateODMWorkflow(ctx context.Context, config *ODMPipelineConfig) (*wfv1.Workflow, error) {
	wf := c.buildODMWorkflow(config)

	created, err := c.wfClientset.ArgoprojV1alpha1().Workflows(c.namespace).Create(
		ctx,
		wf,
		metav1.CreateOptions{},
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create workflow: %w", err)
	}

	return created, nil
}

// buildODMWorkflow constructs the workflow specification
func (c *Client) buildODMWorkflow(config *ODMPipelineConfig) *wfv1.Workflow {
	// Credentials are always required
	if config.S3Credentials == nil {
		panic("S3Credentials must be provided - credentials are required for all S3 operations")
	}

	// Configure AWS/S3 credentials via environment variables
	// Note: We don't use RCLONE_CONFIG_* env vars because ContainerSet filters them
	// Instead, we create rclone config on-the-fly in the scripts using AWS/S3 env vars
	awsEnv := []apiv1.EnvVar{
		// Credentials for S3-compatible access (these are NOT filtered by ContainerSet)
		{Name: "AWS_ACCESS_KEY_ID", Value: config.S3Credentials.AccessKeyID},
		{Name: "AWS_SECRET_ACCESS_KEY", Value: config.S3Credentials.SecretAccessKey},
		{Name: "AWS_DEFAULT_REGION", Value: config.S3Region},
	}
	// Add session token if using STS credentials
	if config.S3Credentials.SessionToken != "" {
		awsEnv = append(awsEnv, apiv1.EnvVar{
			Name:  "AWS_SESSION_TOKEN",
			Value: config.S3Credentials.SessionToken,
		})
	}

	// If a custom S3 endpoint is specified (e.g., for MinIO), expose it as an env var
	if config.S3Endpoint != "" {
		awsEnv = append(awsEnv, apiv1.EnvVar{
			Name:  "AWS_S3_ENDPOINT",
			Value: config.S3Endpoint,
		})
	}

	// Generate unique job ID for this workflow instance
	jobID := "{{workflow.name}}"

	// Download container - downloads from readS3Path and extracts zips
	// Uses include filters to only download image files and archives
	// Logs are written to shared workspace for later collection
	downloadContainer := wfv1.ContainerNode{
		Container: apiv1.Container{
			Name:    "download",
			Image:   config.RcloneImage,
			Command: []string{"/bin/sh", "-c"},
			Args: []string{
				// Redirect output to log file in workspace for later collection
				// Use workflow name template directly in tee path since it's templated by Argo
				s3.GenerateDownloadScript(jobID, config.ReadS3Path) + " 2>&1 | tee /workspace/{{workflow.name}}/.download.log",
			},
			Env: awsEnv,
		},
	}

	// ODM processing container
	// Logs are written to shared workspace for later collection
	odmFlagsStr := strings.Join(config.ODMFlags, " ")
	odmContainer := wfv1.ContainerNode{
		Container: apiv1.Container{
			Name:    "process",
			Image:   config.ODMImage,
			Command: []string{"/bin/bash", "-c"},
			Args: []string{
				fmt.Sprintf(`
set -e
JOB_ID="{{workflow.name}}"
LOG_FILE="/workspace/$JOB_ID/.process.log"
echo "Running ODM processing..." | tee -a "$LOG_FILE"
echo "Processing job: $JOB_ID" | tee -a "$LOG_FILE"
echo "ODM Project ID: %s" | tee -a "$LOG_FILE"
odm_args="%s --project-path /workspace $JOB_ID"
echo "Executing: python3 run.py $odm_args" | tee -a "$LOG_FILE"
python3 run.py $odm_args 2>&1 | tee -a "$LOG_FILE"
echo "ODM processing complete" | tee -a "$LOG_FILE"
				`, config.ODMProjectID, odmFlagsStr),
			},
		},
		Dependencies: []string{"download"},
	}

	// Upload container - uploads results to writeS3Path
	// Logs are written to shared workspace for later collection
	uploadContainer := wfv1.ContainerNode{
		Container: apiv1.Container{
			Name:    "upload",
			Image:   config.RcloneImage,
			Command: []string{"/bin/sh", "-c"},
			Args: []string{
				// Redirect output to log file in workspace for later collection
				// Use workflow name template directly in tee path since it's templated by Argo
				s3.GenerateUploadScript(config.WriteS3Path) + " 2>&1 | tee /workspace/{{workflow.name}}/.upload.log",
			},
			Env: awsEnv,
		},
		Dependencies: []string{"process"},
	}

	// Cleanup container - collects logs and uploads to S3, then workflow will be deleted
	// This runs after upload to preserve logs before workflow cleanup
	cleanupContainer := wfv1.ContainerNode{
		Container: apiv1.Container{
			Name:    "cleanup",
			Image:   config.RcloneImage,
			Command: []string{"/bin/sh", "-c"},
			Args: []string{
				s3.GenerateLogUploadScript(config.WriteS3Path),
			},
			Env: append(awsEnv,
				// Add namespace for log collection
				apiv1.EnvVar{
					Name:  "ARGO_NAMESPACE",
					Value: c.namespace,
				},
			),
		},
		Dependencies: []string{"upload"},
	}

	// Create workflow
	wf := &wfv1.Workflow{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: "odm-pipeline-",
			Namespace:    c.namespace,
		},
		Spec: wfv1.WorkflowSpec{
			Entrypoint:         "main",
			ServiceAccountName: config.ServiceAccount,
			Templates: []wfv1.Template{
				{
					Name: "main",
					Volumes: []apiv1.Volume{
						{
							Name: "workspace",
							VolumeSource: apiv1.VolumeSource{
								EmptyDir: &apiv1.EmptyDirVolumeSource{},
							},
						},
					},
					ContainerSet: &wfv1.ContainerSetTemplate{
						VolumeMounts: []apiv1.VolumeMount{
							{
								Name:      "workspace",
								MountPath: "/workspace",
							},
						},
						Containers: []wfv1.ContainerNode{
							downloadContainer,
							odmContainer,
							uploadContainer,
							cleanupContainer,
						},
					},
				},
			},
		},
	}

	return wf
}

